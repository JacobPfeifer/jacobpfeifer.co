---
title: COVID-19 Data Scraper
date: 2021-08-17
summary: A script to harvest data from the Sedgwick County COVID-19 dashboard.
img: /content/images/sg-co-script.png
alt: Python Script used to harvest data from the Sedgwick County COVID Dashboard
skill: [programming,python]
role: personal project
timeline: spring 2020
---
{% set modal_img = "/content/images/sg-co-script.png" %}
{% set modal_alt = "" %}
{% include "components/lightbox.njk" %} 

<h2 class="font-serif">Background</h2>

<p>Early in the pandemic, Sedgwick County was reporting basic stats by releasing a daily PDF. The total number of cases and deaths were reported, but not much else. To see how many new cases and deaths were reported each day, you needed to compare it with the previous day's PDF. I wanted to better understand my county's trajectory and therefore how I should shape behavior, so I entered the data from the daily PDFs into a Google Sheet. With a few simple formulas, I was able to see the delta each day.</p>

<p>Sedgwick County eventually released a <a target="blank" href="https://sedgwickcounty.maps.arcgis.com/apps/dashboards/7b2b4364a5fa4ba3a015d52450acfe0d">dashboard</a> with more robust data and a few charts. It was easier to see how many cases, hospitalizations, and deaths were reported each day but I wanted to visualize the longer-term trends. To do that, I needed a clean data set. I decided to continue updating my Google Sheet but wanted to find a more efficient way to keep it current.</p>

<h2 class="font-serif">Solution</h2>

<p>Before writing a script I wanted to see what was available to me with minimal effort. Poking through the dev tools I found a <a target="blank" href="https://services7.arcgis.com/McLat6HlPl45bNBv/ArcGIS/rest/services">public directory</a> that hosted the dashboard data. I was able to get almost everything from this directory via an API call, but some data was missing. More on that soon.</p>

{% set modal_img = "/content/images/dev-tools.png" %}
{% set modal_alt = "Using the Chrome Dev Tools to find the URLs the data is coming from" %}
{% include "components/lightbox.njk" %} 

<p><a target="blank" href="https://www.tableau.com">Tableau</a>, the data visualization tool I wanted to use, imports all kinds of databases but only automatically refreshes with Google Sheets. To leverage that functionality, it made sense for me to continue compiling my data there. I used <a target="blank" href="https://github.com/bradjasper/ImportJSON">ImportJSON</a> to import the data from the county directory.</p>

<p>Then I needed to figure out how to get the two attributes I was missing: how many positive and negative tests were reported each day. The only way to get this data was writing a custom web scraper. After doing some research, I decided to use <a target="blank" href="https://www.selenium.dev/selenium/docs/api/py/">Selenium</a> which is written in Python. Selenium runs a browser instance, allowing it to interact with elements and reveal data that is not visible.</p>

<h2 class="font-serif">Deliverable</h2>

<p>10 tutorials followed and 10,000 errors pasted into Google later, I had a working script. I'm sure my code is full of inefficencies, but it does the job. Here's the order of operations:
</p>
<ol>
    <li>Launch a browser window at the dashboard URL</li>
    <li>Expand the range of the chart to include all historical data</li>
    <li>Show the chart data in a table view</li>
    <li>Harvest the Dates, Positive Tests, and Negative Tests</li>
    <li>Orgainze that data into container that can be easily transformed into JSON</li>
    <li>Post that data to JSONbin</li>
</ol>

<div style="width:100%;height:0px;position:relative;padding-bottom:59.900%;"><iframe src="https://streamable.com/e/n2oyxn?loop=0" frameborder="0" width="100%" height="100%" allowfullscreen style="width:100%;height:100%;position:absolute;left:0px;top:0px;overflow:hidden;"></iframe></div>
<p class="text-sm text-gray-500">Script in action</p>
<hr class="mt-0 mb-4">

<h3 class="font-medium font-serif">covid-scraper.py</h2>

{% highlight "python" %}
# Import Relevant Libraries
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
import requests
import os
import time

print('Starting Broswer')
# Start the Selenium Web Driver
CHROMEDRIVER_PATH = '/chromedriver'
driver = webdriver.Chrome(executable_path=CHROMEDRIVER_PATH)
driver.get("https://app.powerbigov.us/view?r=eyJrIjoiYzgwYzFhNTMtMjc4OC00MWM3LTkyYWItZWQ3YTg5ZmRiOGI2IiwidCI6ImE0Yzc1YTkzLTg2MjQtNDBlMS1iYzRhLWE4ZmNlZjhhZDVlOSJ9")

# Adjust the date picker to show all data currently availible. 
def FixDate():
    print('Adjusting date picker')
    date_picker_date = "3/1/2020"
    action = ActionChains(driver)
    date_input_xpath = '//*[@id="pvExplorationHost"]/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[5]/transform/div/div[3]/div/visual-modern/div/div/div[2]/div/div[1]/div/div[1]/input'

    # Wait for the page to load
    wait.until(EC.presence_of_element_located((By.XPATH,date_input_xpath)))

    # Locate the Date Input
    date_picker = driver.find_element(By.XPATH,date_input_xpath)

    # Erase the prefilled date
    # There's probably a more elegant way to do this, but I couldn't get "clear" to work.
    date_picker.click()
    action.double_click(date_picker)
    date_picker.send_keys(Keys.BACKSPACE)

    # Type the earliest possible date and submit
    date_picker.send_keys(date_picker_date)
    time.sleep(1)
    date_picker.send_keys(Keys.RETURN)

# Call Fix Date
FixDate()

# Right click to show context menu and show data as a table.
# Get Date, Positives, and Negatives and put them in a Dictionary
print('Show context menu')
def GetData():
    context_menu_xpath = '//div[@title="Show as a table"]'
    show_table_xpath = '//*[@title="Rolling 14 Day Avg of Positive % Of All Tests"]'
    
    # Wait until the table finishes loading
    wait.until(EC.presence_of_element_located((By.XPATH,show_table_xpath)))

    # Right click to reveal the hidden table
    show_table = driver.find_element(By.XPATH,show_table_xpath)
    action = ActionChains(driver)
    action.context_click(show_table).perform()
    time.sleep(1)

    # Select "Show as Table" from the context menu
    print('Showing Table of Positive and Negatives Tests')
    context_menu = driver.find_element(By.XPATH,context_menu_xpath)
    context_menu.click()
    time.sleep(5)
    
    # Get dates
    print('Getting Dates')
    d = []
    dates = driver.find_elements(By.XPATH,'//div[@class="rowHeaders"]/div/*')
    for row in dates:
        date = row.find_element(By.XPATH,'.//div').text
        d.append(date)

    # Get positives from table
    print('Getting Positives')
    pb = []
    positives = driver.find_elements(By.XPATH,'//div[@class="bodyCells"]/div/div/div[1]/*')
    for row1 in positives:
        positive = row1.text
        pb.append(positive)
    p = pb[: len(pb) - 5]
    
    # Get negatives from table
    print('Getting Negatives')
    nb = []
    negatives = driver.find_elements(By.XPATH,'//div[@class="bodyCells"]/div/div/div[2]/*')
    for row2 in negatives:
        negative = row2.text
        nb.append(negative)
    n = nb[: len(nb) - 5]
    
    # Combine dates, positives, and negatives into a dictionary so JSON can read it
    print('Formatting data into a dictionary')
    entries = dict(zip(d, zip(p, n)))
    return entries
    

# JSON Bin Settings
jsonbin_url = 'https://api.jsonbin.io/v3/b/60bf093592164b68bec4d70a'
headers = {
    'Content-Type': 'application/json',
    'X-Master-Key': '$hidden',
    'X-Bin-Versioning': 'true',
    'name': 'covid-scraper'
}

# Post Data to JSONbin
data = GetData()
print('Uploading Data to JSONbin')
req = requests.put(jsonbin_url, json=data, headers=headers)
print(req.text)

# Close the Browser
driver.quit()
{% endhighlight %}